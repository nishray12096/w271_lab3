---
title: "Lab 3"
author: "Nishant Velagapudi, Bryan Moore, Morris Burkhardt - Overall"
date: "March 19, 2018"
geometry: margin = 1.5 cm
fontsize: 10 pt
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pkg <- c('knitr', 'Hmisc', 'ggcorrplot', 'car','xts','forecast',
         'dplyr', 'ggplot2', 'jtools', 'readr', 'astsa', 'vars', 'dplyr','tseries', 'GGally')
invisible(lapply(pkg, require, character.only = T))
```

# Question 1: E-Commerce Retail Sales as a Percentage of Total Sales
We have a time series that describes the percentage of sales attributed to e-commerce. Modelling this series will require three major steps. First, we will explore and visualize the series to understand potential transformations as well as AR and MA orders. Second, we will find the SARIMA specification with the optimal AIC value. Finally, we will compare various model specifications (informed either through our EDA or our AutoArima results) on the basis of model performance in a hold-out data set. Data up until 2015 will be used for training, with 2015 and 2016 data being used for model selection. We generate a forecast for 2017 data using the best-performing specification trained over all data available.

## Exploration and Visualization

Although it has been dictated that we are to create a SARIMA model, we will use exploratory analysis to show that this is suitable.

```{r}
# loading the data and making the train-test split
ecom = read.csv(file = "ECOMPCTNSA.csv")
ts_ecom = ts(ecom$ECOMPCTNSA, start = c(1999, 4), frequency = 4)
ts_ecom_train = ts(ts_ecom[time(ts_ecom) < 2015], start = c(1999, 4), freq = 4)
ts_ecom_test = ts(ts_ecom[time(ts_ecom) >= 2015], start = c(2015, 1), freq = 4)
head(ts_ecom_train, 13)
#check for missing values
sum(is.na(ts_ecom_train))
#summarize the series
summary(ts_ecom_train)
```

We can see that we have no missing values in this series. The series ranges from 0.7 to 9.5 with a mean of 3.8 and median of 3.6. 

```{r, fig.height=3.2}
plot(ts_ecom_train, xlab = 'Date', ylab = 'E-Commerce Retail Sales',
     main = 'E-commerce Sales: % of Total From 1999-2016')
lines(ma(ts_ecom_train, order = 4, centre = T), col = 'green')
```

Plotting this series shows both a deterministic trend as well as strong seasonality. A MA(4) model of this series smooths out all variance: reinforcing the observation that we have annual seasonality in this series. We next look at autocorrelation and partial autocorrelation functions of the training series.

```{r,fig.height=4}
par(mfrow=c(1,2))
acf(ts_ecom_train)
pacf(ts_ecom_train)
```

We see that the autocorrelations are significant for a large number of lag (16 quarters), further evidence of non-stationarity. The gradual decay without any spikes at seasonal intervals tells us that we will likely need a non-seasonal AR term p but may not need a seasonal AR term P.

Imposing stationarity will require differencing the series. We take first and fourth order differences as well as the first order difference of the fourth order differences. We do this for the non transformed series as well as the log transformed series. 

```{r, warning=FALSE,message=FALSE}
par(mfrow=c(2,2), mai = c(.3, 0.3, 0.3, 0.1))
log_ts_ecom_train = log(ts_ecom_train)
log_ts_ecom_test = log(ts_ecom_test)

plot(diff(ts_ecom_train, lag = 4),xlab=NULL,ylab=NULL,main = "Lag 4 Differenced Series")
plot(diff(diff(ts_ecom_train, lag = 4),lag=1),xlab=NULL,ylab=NULL, 
     main = "Lag 1 Difference of Lag 4 Difference")

plot(diff(log_ts_ecom_train, lag = 1),xlab=NULL,ylab=NULL,main = "Lag 1 Differenced Log Series")
plot(diff(log_ts_ecom_train, lag = 4),xlab=NULL,ylab=NULL,main = "Lag 4 Differenced Log Series")

adf.test(diff(diff(ts_ecom_train, lag = 4),lag=1))$p.value
adf.test(log_ts_ecom_train)$p.value
```

We can see that our difference of differences approach leads to stationarity, as proven by the Augmented Dickey Fuller test. As this series increases in magnitude, the associated seasonal variance also increases, and none of the differences taken above are able to attain visually consistent variance. We thus examine the effect of taking the logarithmic transform of this series prior to differencing.

We can see that the first order difference of the logarithm of the training series has near-constant variance, and also achieves stationarity by the ADF test. We do not require differencing of the log-transformed series to achieve stationarity.

## Modelling

We will start by using the auto.arima function to identify the SARIMA parameters that minimize AIC. We will explore modelling both, the log transformed time series (handled by the argument "lambda=0") and the non-transformed series.

```{r}
log_model_auto <- auto.arima(ts_ecom_train, ic = 'aicc', lambda = 0)
model_auto <- auto.arima(ts_ecom_train, ic = 'aicc')

log_model_auto
model_auto
```

We can see that log transformed order is (0,1,0)(2,1,0)[4], while the non-transformed order is (0,1,1)(0,1,0)[4]. 

We will now fit models using these orders as well as a specification purely informed by our previous EDA for both the logarithm transformed series as well as the raw series.

The spike in the ACF at a lag of 1 quarter suggests a nonseasonal MA(1) (q=1) component and the spikes at intervals of 4 quarters of lag suggest a seasonal MA(1) (Q=1).  Additionally, the spike at at a lag of 1 quarter in the PACF and the spikes at intervals of 4 quarters of lag tells us that a a nonseasonal AR(1) (p=1) component and a seasonal AR(1) (P=1) component are appropriate for our initial model.  Therefore, our initial model will be of the form $ARIMA(1,1,1)(1,1,1)_4$

In the logarithm case, without accounting for seasonality, we observe a somewhat sinusoidal ACF with a PACF function showing no significant spikes beyond lag 4 that has steady variance with a difference of one.

```{r}
manual_model_raw <- Arima(ts_ecom_train, order=c(1,1,1), seasonal=c(1,1,1))
manual_model_log <- Arima(ts_ecom_train, order = c(0,1,4), seasonal=c(0,0,0), lambda=0)
```

We check each of these four models for validity: we use the Shapiro-Wilk and Box-Ljung tests. The former tests for normality of residuals within the training set, while the latter tests for autocorrelation within these same residuals.

```{r}
log_mod_auto_sw <- shapiro.test(log_model_auto $resid)
log_mod_auto_box <-Box.test(log_model_auto $resid, type = "Ljung-Box")

mod_auto_sw <-shapiro.test(model_auto $resid)
mod_auto_box <-Box.test(model_auto $resid, type = "Ljung-Box")

man_mod_raw_sw <-shapiro.test(manual_model_raw $resid)
man_mod_raw_box <-Box.test(manual_model_raw $resid, type = "Ljung-Box")

log_man_mod_sw <-shapiro.test(manual_model_log $resid)
log_man_mod_box <-Box.test(manual_model_log $resid, type = "Ljung-Box")

TestResults <- data.frame(cbind(rbind(log_mod_auto_sw$p.value,mod_auto_sw$p.value, 
                         man_mod_raw_sw$p.value, log_man_mod_sw$p.value),
                         rbind(log_mod_auto_box$p.value, mod_auto_box$p.value, 
                               man_mod_raw_box$p.value, log_man_mod_box$p.value)))

colnames(TestResults) <- c('Shapiro-Wilk', 'Box-Ljung')
rownames(TestResults) <- c('Log, Auto','Raw, Auto','Log, Manual', 'Raw, Manual')

TestResults
```

We compare these models on the basis of root mean square error (RMSE) of predictions in the validation holdout. Visuals of predictions versus actuals help us understand the forecasts.

```{r, warning=FALSE, message=FALSE}
calculate_rmse = function(fcast, test){
  rmse = sqrt(mean((fcast - test)^2))
}

forecast_log_auto = forecast(log_model_auto, h = 8, lambda=0)
forecast_auto = forecast(model_auto, h = 8)
forecast_log_man = forecast(manual_model_raw, h = 8)
forecast_man = forecast(manual_model_log, h = 8, lambda=0)

df_RMSE = data.frame(rbind(calculate_rmse(forecast_log_auto$mean,ts_ecom_test),
           calculate_rmse(forecast_auto$mean,ts_ecom_test), calculate_rmse(
             forecast_log_man$mean, ts_ecom_test), calculate_rmse(forecast_man$mean, ts_ecom_test)))

colnames(df_RMSE) = c('RMSE')
rownames(df_RMSE) = c('Log, Auto','Raw, Auto','Log, Manual', 'Raw, Manual')
df_RMSE

par(mfrow=c(2,2), mai = c(.3, 0.3, 0.3, 0.1))
plot(forecast_log_auto, main="AutoArima Spec - Log Transformed"); lines(ts_ecom_test, col='green')
legend(2000, 9, legend=c('Predicted','Observed'), col=c('blue','red'), lty=1, cex=0.8)
plot(forecast_auto, main="AutoArima Spec - Untransformed"); lines(ts_ecom_test, col='green')
plot(forecast_log_man, main="Manually Specified - Log Transformed"); lines(ts_ecom_test, col='green')
plot(forecast_man, main="Manually Specified - Untransformed"); lines(ts_ecom_test, col='green')

```

The AutoArima specification over the log transformed series has the best RMSE in out of sample comparisons. However, this specification also leads to non-normal residuals and potential autocorrelation in the residuals (as identified by the Shapiro-Wilk and Box-Ljung test in the previous section respectively). Thus, we will go forward with the AutoArima specification over the raw series, which has a comparable RMSE and non-significant results in both of these tests. Visually, these predictions are comparable. We finally use this model, trained over all available data, to predict values for 2017.

```{r, fig.height=3.45}
model_auto_all <- auto.arima(ts_ecom, ic = 'aicc')
plot(forecast(model_auto_all,h = 4), main='Forecasted 2017 E-Commerce Sales as Perc. of Total')
```

# Question 2:
<Intro> We will build a vector autoregressive model to forecast 4 time series.

## Exploration and Visualization

First, we load the time series, display a few values and <???>

```{r}
mts = read.csv(file = "data_2018Spring_MTS.txt", sep=' ')
ts_mts = ts(mts[ , c("series1", "series2", "series3", "series4")], start = c(1947, 1), freq = 12)
ts_mts_train = ts(ts_mts[time(ts_mts) < 1993, ], start = c(1947, 1), freq = 12)
ts_mts_test = ts(ts_mts[time(ts_mts) >= 1993, ], start = c(1993, 1), end = c(1993,12), freq = 12)
head(ts_mts_train)
#missing values
sum(is.na(ts_mts_train))
summary(ts_mts_train)
```

Next, we create a plot of all time series in one single graph.

```{r, fig.width = 10, fig.height = 4}
ts.plot(ts_mts_train, gpars=list(main="Plot of all four series", ylab="Unknown units", col=c('blue','red','green','black')))
legend("topleft",c("Series 1","Series 2","Series 3","Series 4"), lty=1, col = c('blue','red','green','black'), bty='n', cex=.75)
```

In the following plot we look at the distribution of the four time series, as well as pairwise scatterplots and correlation coefficients.

```{r}
df_mts_train = data.frame(as.matrix(ts_mts_train), date=time(ts_mts_train))

ggpairs(df_mts_train[c('series1', 'series2', 'series3', 'series4')], title='Pairwise Scatterplot of series1, series2, series3 and series4')
```


```{r}
adf.test(df_mts_train$series1)$p.value; adf.test(df_mts_train$series2)$p.value
adf.test(df_mts_train$series3)$p.value; adf.test(df_mts_train$series4)$p.value
```

```{r warning=FALSE}
adf.test(diff(df_mts_train$series1))$p.value; adf.test(diff(df_mts_train$series2))$p.value
adf.test(diff(df_mts_train$series3))$p.value; adf.test(diff(df_mts_train$series4))$p.value
```

We skip looking at autocorrelation and partial autocorrelation function for the original time series and go straight to looking at the differenced acf and pacf.

```{r,fig.height=4}
acf(diff(ts_mts_train))
pacf(diff(ts_mts_train))
```


Next, we examine cointegration.

```{r warning=FALSE}
cbind(po.test(df_mts_train[c('series1','series2')])$p.value,
po.test(df_mts_train[c('series1','series3')])$p.value,
po.test(df_mts_train[c('series1','series4')])$p.value,
po.test(df_mts_train[c('series2','series3')])$p.value,
po.test(df_mts_train[c('series2','series4')])$p.value,
po.test(df_mts_train[c('series3','series4')])$p.value)
```


## Modelling

First, we are searching for the order of the VAR model by using the VARselect function. 

```{r}
VARselect(diff(ts_mts_train), type = 'const')$selection
```

BIC selects VAR of order 2! For VAR models we usually use the BIC as criterion! We specify our VAR 2 model and immediately check if the residuals are well behaved. For this, we are  conducting a Portmanteau Test.

```{r}
var = VAR(diff(ts_mts_train), p = 2, type = 'const')
serial.test(var, type = 'PT.asymptotic')$serial$p.value[1]
```

We reject the null hypothesis of no serial correlation for the VAR(2) model. We also conducted this test for VAR(3), VAR(4) and VAR(5). For VAR(3) we received a p-value of around 0.015 and for VAR(4) we received a p-value of 0.08. While the p-value for VAR(4) is the first p-value that does not indicate statistical significance, it is somewhat a borderline case. We therefore decided to go one step further and chose a VAR(5) model - which also happens to be the one that was selected by the AIC. The p-value for the VAR(5) models is about 0.23 and thus far away from statistical significance.

```{r}
var_model = VAR(diff(ts_mts_train), p = 5, type = 'const')
```


## Forecasting

Next, we calculate a forecast based on our model and conduct some in-depth residual diagnostics. 

```{r, fig.width=10, fig.height=6}
diff_forecast = forecast(var_model, h=24)
endvals = tail(ts_mts_train,n=1)
int_1 = cumsum(c(endvals[1], diff_forecast$forecast$series1$mean))
int_2 = cumsum(c(endvals[2], diff_forecast$forecast$series2$mean))
int_3 = cumsum(c(endvals[3], diff_forecast$forecast$series3$mean))
int_4 = cumsum(c(endvals[4], diff_forecast$forecast$series4$mean))

ts_forecast = ts(cbind(int_1, int_2, int_3, int_4), start=c(1993, 1), freq=12)
ts_mts_train_window = window(ts_mts_train, start = c(1985,1))

par(mfrow=c(2,2))
ts.plot(ts_mts_train_window[,1], ts_mts_test[,1], ts_forecast[,1], col=c('black', 'green', 'blue'))
ts.plot(ts_mts_train_window[,2], ts_mts_test[,2], ts_forecast[,2], col=c('black', 'green', 'blue'))
ts.plot(ts_mts_train_window[,3], ts_mts_test[,3], ts_forecast[,3], col=c('black', 'green', 'blue'))
ts.plot(ts_mts_train_window[,4], ts_mts_test[,4], ts_forecast[,4], col=c('black', 'green', 'blue'))
```

## Residual diagnostics

First we plot the residuals.

```{r, fig.height=3, fig.width=8, fig.align="center"}
residuals = as.numeric(ts_mts_test) - window(ts_forecast, start = c(1993,1), end = c(1993,12))
plot(residuals)
```

In our model selection process, we already examined the residuals using an asymptotic Portmanteau Test. We received a p-value of 0.2308 for the VAR(5) model. We also conducted a Breush-Godfrey LM test (commented out code below), which gives us a p-value of 0.9054.

Finally, we conducted a Jarque-Bera test for normality. We test against the null hypothesis that the residuals are normally distributed. We reject the null hypothesis with a highly statistical significant test result. 

```{r}
#serial.test(var_model, type="PT.asymptotic"); serial.test(var_model, type="BG")
normality.test(var_model, multivariate.only = TRUE)$jb.mul$JB$p.value[1,1]
```

Interpretation of the above...


We are aware, that our final VAR model is not satisfactory in terms of fit. The following table however shows that different specifications do not lead to better fit. The table shows an overview of the different model specifications with a multitude of statistical test results as well as AIC and BIC for VAR model orders between 1 and 10.

```{r}
results = data.frame(); orderrange = 1:10
for(i in orderrange) {
  var_iter = VAR(diff(ts_mts_train), p = i, type="both")
  
  PT = serial.test(var_iter, type="PT.asymptotic")$serial$p.value
  BG = serial.test(var_iter, type="BG")$serial$p.value
  norm = normality.test(var_iter, multivariate.only = TRUE)$jb.mul$JB$p.value
  results = rbind(results, cbind(AIC(var_iter), BIC(var_iter), PT, BG, norm))
}
rownames(results) = orderrange
colnames(results) = c('AIC', '        BIC', '    Port. p-val', 
                      ' Breusch-G. p-val', 'Jarque-B p-val')
results
```

